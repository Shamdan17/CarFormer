activation_function: gelu_new
architectures: 
  - GPT2Model
attn_pdrop: 0.1
bos_token_id: 50256
embd_pdrop: 0.1
eos_token_id: 50256
initializer_range: 0.02
layer_norm_epsilon: 1e-05
model_type: gpt2
n_ctx: 1024
n_embd: 256
n_head: 16
n_layer: 4
n_positions: 2048
resid_pdrop: 0.1
summary_activation: null
summary_first_dropout: 0.1
summary_proj_to_labels: true
summary_type: cls_index
summary_use_proj: true
init_from_lm_ckp: false
init_name_or_path: 