defaults:
- default
train_micro_batch_size_per_gpu: 256
gradient_accumulation_steps: 1
optimizer:
    type: Adam
    params:
        lr: 5e-5
        betas: 
            - 0.9
            - 0.999
        eps: 1e-6
        weight_decay: 1e-4
fp16: 
    enabled: true
    initial_scale_power: 11
zero_optimization: false